model_type: t5-3b

tokenizer:
  max_length: 256

train:
  output_dir: './results'
  num_train_epochs: 3000
  per_device_eval_batch_size: 16
  per_device_train_batch_size: 16
  warmup_steps: 500
  weight_decay: 0.01
  logging_dir: './logs'
  save_total_limit: 2
  learning_rate: 1e-4
  gradient_accumulation_steps: 1

log:
  log_interval: 100
  eval_interval: 100

mlflow:
  experiment_name: "z_transformers"
  uri: "file:///home/jovyan/mlruns"